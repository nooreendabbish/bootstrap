% Created 2015-04-20 Mon 10:18
\documentclass[bigger]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{tikz,pgfplots,pgfplotstable, amsmath, xspace,geometry}
\usetikzlibrary{shapes}
\newcommand{\A}{\ensuremath{\mathcal{A}}\xspace}\newcommand{\B}{\ensuremath{\mathcal{B}}\xspace}\newcommand\pa[1]{\ensuremath{\left(#1\right)}}
\providecommand{\alert}[1]{\textbf{#1}}

\title{Recent Developments}
\author{Nooreen Dabbish, Daqian Huang, and Shinjini Nandi}
\date{\today}
\hypersetup{
  pdfkeywords={Weighted Bootstrap, Subsampling, M out of N, Bagging},
  pdfsubject={},
  pdfcreator={Emacs Org-mode version 7.9.3f}}

\begin{document}

\maketitle


\begin{abstract}

\end{abstract}

\section{Recent Developments in Bootstrap Methodology}
\label{sec-1}
\subsection{Introduction}
\label{sec-1-1}

 
This article sets out to ``give a bird's eye overview of the current
state of bootstrap research.'' The authors cover basic ideas with
references to bootstrap literature as well as giving in-depth
explanation and examples of extensions. Topics include parametric
inference using bootstrap stimulations, non-uniform nonparametric
sampling, bootstrap failure, hypothesis testing, bagging, dependent
data and other topics. Our goal is to summarize and highlight the
examples and ideas in the paper, and we conclude with a section on
future directions suggesting what next steps can take this work further.
\subsection{Basic Ideas: Bootstrap approaches to confidence intervals and hypothesis testing}
\label{sec-1-2}

 
 The authors summarize several basic concepts including the empirical
 distributions ($\hat{F}$), nonparameteric (F or estimate $\tilde{F}$)
and parametric (F(y;$\psi$)) notation. In an overview of Bootstrap
approaches 
to confidence intervals and hypothesis testings, the authors describe
how 
non parametric CIs may
be obtained, either through Studentized pivots or the direct use of
bootstrap quantiles.


\begin{center}
\begin{tabular}{lll}
\hline
 Confidence interval method             &  Accuracy       &  Note            \\
\hline
 Bootstrap quantiles                    &  O(1/\sqrt{n})  &                  \\
 Studentized bootstrap                  &  O(1/n)         &                  \\
 Bias-corrected and accelerated (BC_a)  &  O(1/n)         &  Intervals are   \\
                                        &                 &  transformation  \\
                                        &                 &  invariant.      \\
\hline
\end{tabular}
\end{center}



As summarized in Table 1, Studentized pivots that use the bootstrap
estimated variance, V$^{\star}$ can be constructed. An Edgeworth
expansion (similar to a Taylor series, but of a probability density
function) is used to demonstrate the bootstrapped pivots are
consistent estimators for ``smooth'' estimators. They are accurate to 1-$\alpha$ + O(1/n), an
improvement of 1/\sqrt{n}.

\begin{itemize}
\item Nonparametric confidence intervals
\begin{itemize}
\item either Studentized pivots or direct use of quantiles
\item Studentized bootstrap uses estimated var V$^{\star}$ are second-orde
   r accurace 1-$\alpha$+O(n$^{\mathrm{-1}}$)
\item improvement over O(n$^{\mathrm{-1/2}}$)
\item BC$_a$ also second-order but additionally tranformation-invariant.
\end{itemize}
\item Using a pivot
\begin{itemize}
\item avoids the need to modify the samping plan in hypothesis testing
\end{itemize}
\item Model-based bootstrapping
\begin{itemize}
\item when data are not identically distributed.
\item time-series/autoregressive moving average
\item Notes on Notation:
\begin{itemize}
\item $\hat{F}$ empirical distribution
\item F(y;$\psi$) parametric model with parameter $\psi$
\item 
\end{itemize}
\item Topics to explore/look-up
\begin{itemize}
\item conditions under which bootstrap is consistent (Bickel and
    Freedman 1981)
\item Edgeworth correction
\item Edgeworth expansion
\item permutation tests
\end{itemize}
\end{itemize}
\end{itemize}
\subsection{Bootstraps for Parametric Likelihood Inference}
\label{sec-1-3}


\begin{itemize}
\item profile log-likelihood l$_p$($\gamma$)
\item ratio statistic $w_p(\gamma) = 2(l_p(\hat{\gamma})-l_p(\gamma)) \sim
  \chi^2_1 + O(n^{-1})$
\item signed root likelihood ratio statistic $$r_p =
  sgn(\hat{\gamma}-\gamma)w_p(\gamma)^{1/2} \sim N(0,1) +
  O(n^{-1/2})$$
\item r$_a$ adjusted r$_p$ $$r_a = r_p + r_p^{-1}\log(u_p/r_p) \sim N(0,1) +
  O(n^{-3/2})$$ where u$_p$ depends on an ancillary statistic.
\item Can reduce error with parametric bootstrapping
\end{itemize}
\begin{itemize}

\item Example 1 Exponential Regression\\
\label{sec-1-3-1}%
Lawless(1982)


\begin{center}
\begin{tabular}{rr}
 Survival in weeks  &  log wbc  \\
\hline
               156  &     2.88  \\
               108  &     4.02  \\
               143  &     3.85  \\
                56  &     3.97  \\
                 1  &      5.0  \\
\hline
\end{tabular}
\end{center}



\begin{itemize}
\item Exponential regression model: lifetimes T$_1$,\ldots,T$_n$,
  independent, E(T$_i$) = exp($\beta$ + $\xi$ z$_i$)
\item want mean lifetime for z$_0$, parameter of interest $\gamma$ = $\beta$ +
  $\xi$ z$_0$
\item r$_p$ is exactly pivotal $\rightarrow$ bootstrap yields true sampling
  distribution, 1-$\alpha$ coverage (opposed to coverage error O(1/n)).
\end{itemize}


\item When r$_p$ is not exactly pivotal
\label{sec-1-3-2}%
\begin{itemize}
\item can improve errors using constrained estimator
   $(\gamma,\hat{\xi_\gamma})$, to give r$_p$^\dag with error rate
   O(n$^{\mathrm{-3/2}}$).
\item Robbins-Monro stochastic search algorithm makes this possible
  wit a single bootstrap sample for each value $\gamma$ takes on.
\end{itemize}


\item Example 2: Normal distributions with common mean, different variances\\
\label{sec-1-3-3}%
\begin{itemize}
\item r$_a$ is intractable
\item strength measurements in six sample of cotton yarn
\item used contstrained mle and simple search algorithm
\end{itemize}

\end{itemize} % ends low level
\subsection{Weighted Non-parametric Bootstrapping}
\label{sec-1-4}


\begin{itemize}
\item ordinary--uniform resampling
\item null hypothesis sample from $\tilde{F_0}$, constrained by
  $\theta(\tilde{F_0}) = \theta_0$
\item $\hat{F_p}$ distribution that gives prob p$_i$ to Y$_i$
\item minimize Kullback-Leibler distance
\item resample $\hat{F_p}$ to get $Y^\dag$
\end{itemize}
\begin{itemize}

\item Example 3: Folded normal mean\\
\label{sec-1-4-1}%
Study to compare the coverage properties of bootstrap confidence
intervals for mean $\mu$ of folded standard normal.



\begin{itemize}
\item Things to look up:
\end{itemize}
++ How uniform bootstrapping reduces error by O(n$^{\mathrm{-1/2}}$)


\end{itemize} % ends low level
\subsection{Subsampling and the m out of n bootstrap}
\label{sec-1-5}


\begin{itemize}
\item goal is to provide asymptotic consistency when bootstrap fails
\item subsamples calculated without replacement
\item ``m out of n bootstrap'' size m<n or m<<n
\end{itemize}

Unless we have some rigid conditions, bootstrap cannot give the
second-order accuracy. We can get better results asymptotically from
subsampling. Subsampling theory says that we draw the samples without
replacement. M out of N says you can draw the subsamples, without
replacement.  

\begin{itemize}
\item Y random sample from F
\item wish to construct confidence set for $\theta$
\item J(F) non-degenerate limiting distribution
\end{itemize}

If we find L, the empirical distribution function from the
subsamples, we can show that it converges to the limiting
distribution very well.

\begin{itemize}
\item time series: subsampling remains valid under strong mixing
  assumption.
 So if it is time series, we have to choose a subsample sequentially.
  Everything holds, but the subsample for example of Y$_1$, \ldots,
  Y$_{\mathrm{10}}$, could have been 1 and 4 then 2, but if it is time series you
  have to choose them in order. One way to accomplish this is to
  choose the first element randomly and the remaining items
  sequentially.
\item extra conditions are not required for subsampling
\end{itemize}
Bootstrap also gives similar consistency, under stricter conditions.
It works under weak conditions as well. Under strong conditions,
using a metric, you can prove bootstrap consistency. These are not
necessary for subsampling.

\begin{itemize}
\item m out of n:  asymptotic validity also true
\end{itemize}
m out of n is with replacement, subsampling is without replacement.
You can perform m out of n when m$^2$/n $\rightarrow$ 0. If that holds
true, both methods are the same. Otherwise, subsampling gives better
results (m out of n will not be asymptotically consistent).

\begin{itemize}
\item choice of subsample size
\item level of accuracy
\end{itemize}
The authors cite an example of concern about the level of accuracy in
which the normal approximation is better than subsampling.

\begin{itemize}
\item summary
\end{itemize}
When the advantages of subsampling are clear, we prefer it out of
ordinary or m out of n bootstrap. If you are unsure of the validity
of bootstrap, use subsampling. If m out of n is valid, it is
preferrable.
\subsection{Bootstrapping Superefficient estimators}
\label{sec-1-6}
\begin{itemize}

\item Consistency  of convential bootstrap depends on true value of $\theta$\\
\label{sec-1-6-1}%
\begin{itemize}
\item Beran (1997)
\end{itemize}


\item Stein estimator\\
\label{sec-1-6-2}%
\begin{itemize}
\item prototypical of many nonparametric smoothers
\item $Y_1, \ldots, Y_n \overset{iid}{\sim} N_k(\theta,I)$
\end{itemize}

$$T = \left( 1 - \frac{k-2}{n||\bar{Y}||^2}\right)\bar{Y}$$


\item Example 4 (Stein Estimator)
\label{sec-1-6-3}%


\item Fig. 1  Empirical Coverages of Confidence Sets in the Stein Estimator Example
\label{sec-1-6-4}%

\end{itemize} % ends low level
\subsection{More on Significance Tests}
\label{sec-1-7}


\begin{itemize}
\item nonparametric model fit tests, comparing to parametric model fits
  or fits from several data sets
\item ESP: Empirical strength probability, with H$_0$: $\theta$ $\in$ $\Theta$$_0$,
\end{itemize}
ESP = proportion of $\tilde{\theta^{\star}_r} \in \Theta_0$.

ESP acts like a p-value asymptoticall as n$\rightarrow$ $\infty$.
\begin{itemize}

\item Example 5 Exponential Mean\\
\label{sec-1-7-1}%
\begin{itemize}
\item Y$_i$ independent $\sim$ exp($\mu$\}
\item H$_0$: $\mu$ \leq $\mu$$_o$, H$_1$: $\mu$ > $\mu$$_0$
\item exact test has a p $\sim$ U
\item Parametric bootstrap does not work well for small sample,
  nonparametric works well.
\end{itemize}


\item Example 6 Poisson Dispersion\\
\label{sec-1-7-2}%
\begin{itemize}
\item test for Poisson Dispersion or overdispersion: Y$_1$,\ldots,Y$_n$ iid ($\mu$,$\sigma$$^2$),
  H$_0$:$\theta$=$\mu$/sigma$^2$ \geq 1, H$_1$: $\theta$ < 1.
\end{itemize}


\item Point null hypothesis, Distribution testing\\
\label{sec-1-7-3}%
\begin{itemize}
\item point null define a confidence set for $\theta$.
\item To test a specific distribution, define distance between
  distributions and use d($\tilde{F}$,F$_0$)
\end{itemize}

\end{itemize} % ends low level
\subsection{Bagging and Classification}
\label{sec-1-8}
\begin{itemize}

\item Background on classification
\label{sec-1-8-1}%

\begin{frame}
\frametitle{A two-predictor problem}
\label{sec-1-8-1-1}


\pgfmathsetseed{1138} % set the random seed
\pgfplotstableset{ % Define the equations for x and y
    create on use/x/.style={create col/expr={5.5+2.4*rand}},
    create on use/y/.style={create col/expr={5+1.5*rand}}
}
% create a new table with 30 rows and columns x and y:
\pgfplotstablenew[columns={x,y}]{30}\testtable
\pgfplotstableset{ % Define the equations for x and y
    create on use/x/.style={create col/expr={1.5+rand}},
    create on use/y/.style={create col/expr={2+1.3*rand}}
}
% create a new table with 30 rows and columns x and y:
\pgfplotstablenew[columns={x,y}]{30}\testtabletwo
\pgfplotstableset{ % Define the equations for x and y
    create on use/x/.style={create col/expr={2.5+rand}},
    create on use/y/.style={create col/expr={6+3*rand}}
}
% create a new table with 30 rows and columns x and y:
\pgfplotstablenew[columns={x,y}]{30}\testtablethree

\begin{tikzpicture}
\begin{axis}[
xlabel=Predictor A, % label x axis
ylabel=Predictor B, % label y axis
axis lines=left, %set the position of the axes
xmin=0, xmax=10, % set the min and max values of the x-axis
ymin=0, ymax=10, % set the min and max values of the y-axis
clip=false
]

\addplot [only marks, mark=star, green!30!black] table {\testtable};
\addplot [only marks, mark=diamond, blue] table {\testtabletwo};
\addplot [only marks, mark=square*, red] table {\testtablethree};

\end{axis}

\end{tikzpicture}
\end{frame}
\begin{frame}
\frametitle{A classification scheme}
\label{sec-1-8-1-2}

\begin{minipage}{.475\textwidth}
\includegraphics[width=0.5\textwidth]{\begin{tikzpicture}[
    thick,
    >=stealth',
    dot/.style = {
      draw,
      circle,
      inner sep = 0pt,
      minimum size = 4pt
    }
  ]
  \coordinate (O) at (0,0);
  \draw[-] (-0.3,0) -- (10.3,0) coordinate[label = {below:Predictor A}] (xmax);
  \draw[-] (0,-0.3) -- (0,10.3) coordinate[label = {right:Predictor B}] (ymax);
  \draw[-] (-0.3,10) -- (10.3,10);
  \draw[-] (10,-0.3) -- (10,10.3);
        %ticks
        \foreach \x in {0,...,9}
                \draw (\x,1pt) -- (\x,-3pt)
                        node[anchor=north] {\x};
        \foreach \y in {0,...,10}
                \draw (1pt,\y) -- (-3pt,\y) 
                        node[anchor=east] {\y};
        \foreach \x in {0,...,10}
                \draw (\x,9.9) -- (\x,10.1);
        \foreach \y in {0,...,10}
                \draw (9.9,\y) -- (10.1,\y);
  
  \draw[-,red] (4.0,0) -- (4.0,10);
  \draw[-,blue] (0,2.5) -- (4, 2.5);
  
  \node[green!30!black] at (7,5) {Class 1};
  \node[blue] at (1.5,1.5) {Class 2};
  \node[red] at (1.5,6) {Class 3};
 
\end{tikzpicture}
\end{minipage}
\begin{minipage}{.475\textwidth}
\begin{tikzpicture}
[-,thick]
\node {A >= 4}
  [sibling distance=2.5cm]
 child {node {B < 2.5}
    [sibling distance=2cm]
    child {node {Class 3}}
    child {node {Class 2}}
  } 
 child {node {Class 1}
   };
 \end{tikzpicture}
\end{minipage}



\begin{itemize}
\item ``bagging''--bootstrap aggregation, uses resampling as a smoothing device
\begin{itemize}
\item prediction and nonparametric classification problems
\item useful when basic algorithm is unstable after small data perturbations
\end{itemize}
\item empirical bagged predictor, acts as asmoother which reduces
  variance.
\item can reduce MSE of predictor by 50\%
\item bagging by voting: pick the class which is choosen most often in R
  resamples
\item ``boosting'' attaches weights to the data according to difficulty clasifying.
\end{itemize}
\end{frame}
\end{itemize} % ends low level
\subsection{Bootstrapping Dependent Data}
\label{sec-1-9}
\subsection{Other topics}
\label{sec-1-10}
\subsection{Final Remarks}
\label{sec-1-11}
\section{Future Directions}
\label{sec-2}
\section{Bagging notes}
\label{sec-3}
\subsection{Breiman 1996}
\label{sec-3-1}



\begin{center}
\begin{tabular}{ll}
 Stable             &  Unstable                               \\
\hline
 k-nearest neighor  &  Neural nets                            \\
                    &  Classification trees                   \\
                    &  Regression trees                       \\
                    &  Subset selection in linear regression  \\
\hline
\end{tabular}
\end{center}



\begin{itemize}
\item Classification tree examples
\end{itemize}


\begin{verbatim}
library(rpart) 
glass <- read.csv("glass.data.txt")

colnames(glass) <- c("Id","RefIndex","Na","Mg","Al","Si","K","Ca","Ba","Fe","Type")
##Type of glass: (class attribute)
#      -- 1 building_windows_float_processed
#      -- 2 building_windows_non_float_processed
#      -- 3 vehicle_windows_float_processed
#      -- 4 vehicle_windows_non_float_processed (none in this database)
#      -- 5 containers
#      -- 6 tableware
#      -- 7 headlamps


r01 <- sample(c(0,1), 213, replace=TRUE)


glass.L <- glass[r01 == 0,]
glass.T <- glass[r01 == 1,]

dim(glass.L)
dim(glass.T)

glass.tree <- rpart(X1.1 ~X0.00.1+X0.00+X8.75+X0.06+X71.78+X1.10+X4.49+X13.64+X1.52101,
                    data = glass.L, method="class")

#

plot(glass.tree)
text(glass.tree, use.n=TRUE, all=TRUE, cex=.8)
?predict

pred <- predict(glass.tree, glass.T)
pred
pred == glass.T[, ncol(glass.T)]
pred
\end{verbatim}

\end{document}
